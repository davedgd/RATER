{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08242742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daved/RATER-C\n"
     ]
    }
   ],
   "source": [
    "%cd ~/RATER-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98625c55-290e-41d8-9ca0-5393c1ecbe3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f485468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import set_seed\n",
    "seed = 123\n",
    "\n",
    "set_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f237445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daved/miniforge3/envs/ht/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'models/microsoft_deberta-v3-large'\n",
    "pretrained_model_name_or_path = model_checkpoint.split('models/')[1].replace('_', '/')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels = 2\n",
    ").to(device)\n",
    "\n",
    "pipe = TextClassificationPipeline(model = model, tokenizer = tokenizer, top_k = None, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f22b7e2-29d0-4a39-8c41-6c752f80df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.read_excel(\"data/raw/Highest domain-fixed.xlsx\", sheet_name = 2) # sheet 2 for Study 1; sheet 3 for Study 2\n",
    "pred_df = pd.read_excel(\"data/raw/Sample data sheet for Kai v9.xlsx\")\n",
    "\n",
    "human_sheet = pred_df.merge(cm_df, on = 'Match ID', how = 'right')\n",
    "human_sheet['Full construct'] = human_sheet['Construct name'] + ': ' + human_sheet['Construct definition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f971f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The extent to which the recruiter and the job applicant share important values, attitudes, and beliefs.',\n",
       " 'text_pair': 'The job applicant and I are similar in terms of our outlook, perspective, and values.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test_dict = []\n",
    "\n",
    "for i, _ in human_sheet.iterrows():\n",
    "    data_test_dict.append({'text': human_sheet['Construct definition'][i], \n",
    "                           'text_pair': human_sheet['Item text'][i]})\n",
    "\n",
    "data_test_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80049806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary workaround for XLNet batch size issue\n",
    "if str(model.base_model).find('XLNetModel') != -1:\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 128\n",
    "\n",
    "raw_probs = pipe(data_test_dict, batch_size = batch_size)\n",
    "\n",
    "probs = np.array([item[['LABEL_1' == i['label'] for i in item].index(True)]['score'] for item in raw_probs])\n",
    "preds = np.where(probs >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "399f5033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_sheet['Model Prob'] = probs\n",
    "human_sheet['Model Pred'] = preds\n",
    "\n",
    "len(human_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4925411",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_sheet[['Domain HT']] = human_sheet[['Domain HT']].replace(0.0, '')\n",
    "human_sheet[['Domain v6']] = human_sheet[['Domain v6']].replace(0.0, '')\n",
    "human_sheet[['Domain loading']] = human_sheet[['Domain loading']].replace(0.0, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb8bb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix (sheet, ground_truth = 'author', final_loading = None, grouped = False):\n",
    "\n",
    "    sheet = sheet.copy()\n",
    "\n",
    "    sheet['Final loading'] = final_loading\n",
    "    \n",
    "    if ground_truth == 'author':\n",
    "        target_df = sheet.query('Target_y == 1')\n",
    "        non_target_df = sheet.query('Target_y == 0')\n",
    "        \n",
    "        tp = sum(target_df['Final loading'] == target_df['Construct ID'])\n",
    "        fn = sum(target_df['Final loading'] != target_df['Construct ID'])\n",
    "        fp = sum(non_target_df['Final loading'] == non_target_df['Construct ID'])\n",
    "        tn = sum(non_target_df['Final loading'] != non_target_df['Construct ID'])\n",
    "        \n",
    "    elif ground_truth == 'ht':\n",
    "        target_df = sheet[sheet['Domain HT'] != '']\n",
    "        non_target_df = sheet[sheet['Domain HT'] == '']\n",
    "\n",
    "        tp = sum(target_df['Final loading'] == target_df['Domain HT'])\n",
    "        fn = sum(target_df['Final loading'] != target_df['Domain HT'])\n",
    "        fp = sum(non_target_df['Final loading'] != non_target_df['Domain HT'])\n",
    "        tn = sum(non_target_df['Final loading'] == non_target_df['Domain HT'])\n",
    "    elif ground_truth == 'domain':\n",
    "        sheet = sheet[~sheet['Domain loading'].isna()].copy()\n",
    "\n",
    "        target_df = sheet[sheet['Domain loading'] != '']\n",
    "        non_target_df = sheet[sheet['Domain loading'] == '']\n",
    "\n",
    "        tp = sum(target_df['Final loading'] == target_df['Domain loading'])\n",
    "        fn = sum(target_df['Final loading'] != target_df['Domain loading'])\n",
    "        fp = sum(non_target_df['Final loading'] != non_target_df['Domain loading'])\n",
    "        tn = sum(non_target_df['Final loading'] == non_target_df['Domain loading'])\n",
    "\n",
    "    print('Total Data Rows:', len(sheet))\n",
    "    \n",
    "    print(np.array([[tp, fp], [fn, tn]]))\n",
    "    \n",
    "    Accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    Sensitivity = tp / (tp + fn)\n",
    "    Specificity = tn / (tn + fp)\n",
    "    PrecPos = tp / (tp + fp)\n",
    "    PrecNeg = tn / (tn + fn)\n",
    "    F1 = (2 * Sensitivity * PrecPos) / (Sensitivity + PrecPos)\n",
    "\n",
    "    P_o = (tp + tn) / (tp + tn + fp + fn)\n",
    "    total = tp + tn + fp + fn\n",
    "    P_e = ((tp + fp) * (tp + fn) + (tn + fn) * (tn + fp)) / (total ** 2)\n",
    "    Kappa = (P_o - P_e) / (1 - P_e)\n",
    "    \n",
    "    if grouped:\n",
    "        print('Source ID:', np.unique(sheet['Source ID'])[0])\n",
    "    print('Accuracy:', np.round(Accuracy * 100, 0))\n",
    "    print('Error Rate:', np.round(100 - Accuracy * 100, 0))\n",
    "    print('Recall of positive class (sensitivity):', np.round(Sensitivity * 100, 0))\n",
    "    print('Recall of negative class (specificity):', np.round(Specificity * 100, 0))\n",
    "    print('Precision of positive class:', np.round(PrecPos * 100, 0))\n",
    "    print('Precision of negative class:', np.round(PrecNeg * 100, 0))\n",
    "    print('F-Measure:', np.round(F1, 2))\n",
    "    print('Cohen\\'s Kappa:', np.round(Kappa, 2))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eb5f8ca-8e32-48ce-a633-6f394698edaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT: H&T ratings\n",
      "Total Data Rows: 467\n",
      "[[138  21]\n",
      " [ 21 287]]\n",
      "Accuracy: 91.0\n",
      "Error Rate: 9.0\n",
      "Recall of positive class (sensitivity): 87.0\n",
      "Recall of negative class (specificity): 93.0\n",
      "Precision of positive class: 87.0\n",
      "Precision of negative class: 93.0\n",
      "F-Measure: 0.87\n",
      "Cohen's Kappa: 0.8\n",
      "---\n",
      "RESULT: Model Prob\n",
      "Total Data Rows: 467\n",
      "[[131  28]\n",
      " [ 28 280]]\n",
      "Accuracy: 88.0\n",
      "Error Rate: 12.0\n",
      "Recall of positive class (sensitivity): 82.0\n",
      "Recall of negative class (specificity): 91.0\n",
      "Precision of positive class: 82.0\n",
      "Precision of negative class: 91.0\n",
      "F-Measure: 0.82\n",
      "Cohen's Kappa: 0.73\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for each in ['H&T ratings', 'Model Prob']:\n",
    "    print('RESULT:', each)\n",
    "\n",
    "    calc_loading = human_sheet.loc[human_sheet.groupby('Item ID')[each].rank(method = 'min', ascending = False) == 1, 'Construct ID'].reindex(human_sheet.index, fill_value = '')\n",
    "    human_sheet['Final loading - ' + each] = calc_loading\n",
    "\n",
    "    conf_matrix(human_sheet, ground_truth = 'author', final_loading = calc_loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03f49e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_sheet = human_sheet.merge(pd.DataFrame(human_sheet.groupby('Item ID')['Model Prob'].mean()).reset_index().rename(columns = {'Model Prob': 'Mean Model Prob'}), how = 'left')\n",
    "human_sheet.drop(['Reversed_x', 'Target_x', 'P match', 'P match v1', 'P match v2', 'P match v3', 'P match v4', 'LSACosT2T', 'LSACosD2D', 'Word2VecCos', 'BERTCos', 'Full construct'], axis = 1).to_csv('data/analysis/model_results_' + str(len(human_sheet)) + '.csv', index = False)\n",
    "\n",
    "len(human_sheet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ht",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
