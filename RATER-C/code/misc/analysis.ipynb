{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daved/RATER-C\n"
     ]
    }
   ],
   "source": [
    "%cd ~/RATER-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daved/miniforge3/envs/ht/lib/python3.10/site-packages/textstat/textstat.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/home/daved/miniforge3/envs/ht/lib/python3.10/site-packages/pkg_resources/__init__.py:3146: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import janitor\n",
    "\n",
    "import textstat # using 0.7.4\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import set_seed\n",
    "seed = 123\n",
    "\n",
    "set_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daved/miniforge3/envs/ht/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'models/microsoft_deberta-v3-large'\n",
    "pretrained_model_name_or_path = model_checkpoint.split('models/')[1].replace('_', '/')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels = 2\n",
    ").to(device)\n",
    "\n",
    "pipe = TextClassificationPipeline(model = model, tokenizer = tokenizer, top_k = None, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_excel('data/processed/train_val_test.xlsx', sheet_name = 'test')[['definition', 'ItemText', 'Target']].clean_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Participants either heard the instructions and scale items and responded by speaking their response into a microphone, or they read this information and responded by typing their response.',\n",
       " 'text_pair': 'Do you think that __ is very good, pretty good, neither good nor bad, pretty bad, or very bad?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test_dict = []\n",
    "\n",
    "for i, _ in test.iterrows():\n",
    "    data_test_dict.append({'text': test['definition'][i], \n",
    "                           'text_pair': test['itemtext'][i]})\n",
    "\n",
    "data_test_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary workaround for XLNet batch size issue\n",
    "if str(model.base_model).find('XLNetModel') != -1:\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 128\n",
    "\n",
    "raw_probs = pipe(data_test_dict, batch_size = batch_size)\n",
    "\n",
    "probs = np.array([item[['LABEL_1' == i['label'] for i in item].index(True)]['score'] for item in raw_probs])\n",
    "preds = np.where(probs >= 0.5, 1, 0)\n",
    "\n",
    "y_true = test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.911"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(roc_auc_score(y_true, probs), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.782"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(f1_score(y_true, preds, average = 'macro'), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dat = pd.DataFrame({\n",
    "    'definition': test['definition'],\n",
    "    'itemtext': test['itemtext'],\n",
    "    'y_true': y_true,\n",
    "    'preds': preds,\n",
    "    'probs': probs,\n",
    "    'correct': y_true == preds,\n",
    "    'flesch_reading_ease': [textstat.flesch_reading_ease(x) for x in test['definition'] + ' ' + test['itemtext']],\n",
    "    'gunning_fog': [textstat.gunning_fog(x) for x in test['definition'] + ' ' + test['itemtext']],\n",
    "    'consensus': [textstat.text_standard(x) for x in test['definition'] + ' ' + test['itemtext']],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dat.to_csv('data/analysis/analysis.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ht",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
